# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11TK1xmybI7ja8j33JoSkcs_m4jHax7fA
"""

# !pip install langchain langchain_openai chromadb streamlit unstructured python-pptx python-docx pypdf google-auth-oauthlib google-auth-httplib2 google-api-python-client langchain_community PyMuPDF docx2txt google-cloud-storage google.cloud tenacity



import os
import tempfile
import streamlit as st
import io
import base64
from typing import List, Tuple
import fitz
import docx2txt
from tenacity import retry, stop_after_attempt, wait_exponential
from google.cloud import storage

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.schema import Document
from langchain.chains import ConversationalRetrievalChain

import pypdf
import json
from pptx import Presentation
from docx import Document

BUCKET_NAME = os.environ.get("BUCKET_NAME")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
TYPE = os.environ.get("TYPE")
PROJECT_ID = os.environ.get("PROJECT_ID")
PRIVATE_KEY_ID = os.environ.get("PRIVATE_KEY_ID")
PRIVATE_KEY = os.environ.get("PRIVATE_KEY")
CLIENT_EMAIL = os.environ.get("CLIENT_EMAIL")
CLIENT_ID = os.environ.get("CLIENT_ID")
AUTH_URI = os.environ.get("AUTH_URI")
TOKEN_URI = os.environ.get("TOKEN_URI")
AUTH_PROVIDER_CERT = os.environ.get("AUTH_PROVIDER_CERT")


import streamlit
import json
    
service_account_info = {
    "type" = "service_account",
    "project_id": "daclub-session-1",
    "private_key_id": "dc01ff3c6d344059dee7e6892ee7995cc1acf06d",
    "private_key": "MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC3hhlixFeDI4E1\n6bXnlB46C9eQV7MZEjaIVTsDFs8x/PlIfgNqNgRa5BFcwWswA6VjTHpmj0wgQxQq\n8Dt1y121G3rF6wEdfBfH9UPCjGqxVVzk4EyK8+jUoE4TTALWEOBoITbebd0+PVEa\nztje+48fa5QCdxCM9hMq/trSN8tYgz8ZYCY/a7JVhfY4kp2azJV8iTMtdANWjP2J\nVzrdDhOqxHoQ5+zk+3pqgkyFerkyJ61NACDX89/ghWvhFFKq1A2yp2zamr5In8Rr\nSUbB85Qd6uGXzKcLj++vWdC9dXPzo10kEup85NLYD6SdIzI6uhpBJj9zHQAdb5/F\n7pdHddhXAgMBAAECggEAMefb8GE/APC/OsduiWZcX+GHgX9vuwIaZTF0Ji9mMYw+\nx6lIyXGd9o9tb0FDoh9jsqCB6nDOPTmwesqwCKTAOzcZSEfTGk778oquHfHsM4Po\nim66MdEJAAUFpja10LtvIrHhh1Pt5XZsHzOfgud6cItjCNaS4BosdTUvkRnOCKfW\nFweRU0BTd01IjK2pyJ1pxZdhU5BKq1jAMBqy+Gyqc85sJ3Vl4XsSvRW094C/Gw8R\nYjQwPKM4m4QnY9DxYhmDgwaylU2eh01hcp2TBPLx1Pnn0avzU0DltMWlCBiuThUl\n9kuUcOlCCZdk+Sfs4zzlvOE9GCqsvVw9aGOJHE06EQKBgQDi5tdd723jOosemEol\nsBLYieLPVYFKlE5yl4w1QJbmrCiczXKKOi/RYPf17D+iGd2B0uFfhVyQYab6nCZ5\ngxSo9zc3kcd7msLdJjKxRLq9YnSBffj6tBEh2bgNx/y1ilGX8uz3QjAT0QxAAeR1\n6EJQ5mTgR/BZ/mafflIQwYf6EQKBgQDPDyqNZCSVVC7+0EA3al/Wob72Q3HH9GDP\nJmmXq13ufokEt8ERaHBIkazXavzllSjJg2Q5O/XKZ0rwry/djNyuc3uP1QkwF6cg\nKxwBMfOgMqvhKpYoE31h7LX7jzTdbZnLSmS/wsXE4hb/9ao50RahpYYYp2HBpo8X\naYUh84UD5wKBgFWcEVAw4T4lvY3KmddEaqfmxnvB4ClacYrM4SrKduULubsHzRqY\nP6h5NLVF8PBhX/D2tvAKalVTcuQFfILGUUo7FEtUJnbden5kRiYaL3b7Kcgd1o4I\n2Jn0Any4owF/8a8qCfx0rZ0cd8nCrQdLnZHWi+MPYYEi+mddCuuliX9xAoGBALQ5\nHC2TM7ByZLFP7AUP5rk3cbKBw8Bp+fV0FsZ31ztmEbb0heyz/b98ZHff3J2r9aNo\nZMzYXri8rWLO6Reqqs6BH9RydhvRCpuaSjbUFpyiXi6PVcEuLZ0VXtTFshrlcc6b\nQxQR4PfcXm6NkovebRdnrk85KYJXigazxrhlcKqTAoGAMkg4IBp7TWqpOx/lljOn\nVsu+GnmGtBD+hLM6rTs0Wk4X1iOijnDGMa/XM76Us9LGswaALJ4cnV62gOoINBmL\nqGqHmxvCs+cSIu7HrpPM0307h55fhv94N51SBLUSKrByZ68HZGGyu6TJGv3mRG4l\nm66lZBrPKvuE0sM0E6YcSSo=",
    "client_email": "rag-chatbot@daclub-session-1.iam.gserviceaccount.com",
    "client_id": "108357860878317518131",
    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
    "token_uri": "https://oauth2.googleapis.com/token",
    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
    "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/rag-chatbot%40daclub-session-1.iam.gserviceaccount.com",
    "universe_domain": "googleapis.com"
}

def initialize_gcs_client():
    """Initialize Google Cloud Storage client with service account"""
    storage_client = storage.Client.from_service_account_info('service_account_info')
    return storage_client

def download_files_from_bucket(storage_client) -> List[Tuple[str, bytes]]:
    """Download all files from the specified GCS bucket"""
    bucket = storage_client.bucket(BUCKET_NAME)
    blobs = bucket.list_blobs()

    downloaded_files = []
    for blob in blobs:
        content = blob.download_as_bytes()
        downloaded_files.append((blob.name, content))
        print(f"Downloaded: {blob.name}")

    return downloaded_files

def process_file(file_name: str, content: bytes) -> str:
    """Process different file types and extract text"""
    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(content)
        temp_path = temp_file.name

    text = ""
    try:
        if file_name.endswith('.pdf'):
            # Use PyMuPDF for PDFs
            doc = fitz.open(temp_path)
            for page in doc:
                text += page.get_text()
            doc.close()

        elif file_name.endswith('.docx'):
            # Use docx2txt for DOCX files
            text = docx2txt.process(temp_path)

        elif file_name.endswith('.pptx'):
            prs = Presentation(temp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"

    finally:
        os.unlink(temp_path)

    return text

def initialize_qa_system():
    """Initialize the QA system with the configured bucket and API key"""
    # Initialize OpenAI components
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    llm = ChatOpenAI(
        temperature=0,
        openai_api_key=OPENAI_API_KEY,
        model="gpt-4-turbo-1106"
    )

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
    def embed_with_retry(texts):
        return embeddings.embed_documents(texts)

    # Download and process files from GCS
    storage_client = initialize_gcs_client()
    files = download_files_from_bucket(storage_client)

    print(f"Files found: {[f[0] for f in files]}")

    all_text = ""
    for file_name, content in files:
        print(f"Processing: {file_name}")
        all_text += process_file(file_name, content) + "\n\n"

    print(f"All text length: {len(all_text)}")

    # Split text into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    texts = text_splitter.create_documents([all_text])

    # Create vector store
    vectorstore = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory="chroma_db"
    )

    # Set up retriever with reranking
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    compressor = LLMChainExtractor.from_llm(llm)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    # Create conversational chain
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=compression_retriever,
        return_source_documents=True,
        verbose=True
    )

    return qa_chain

def add_logo():
    """Add logo to the Streamlit interface"""
    logo_svg = '''
    <svg width="100" height="100" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
        <rect x="20" y="20" width="60" height="60" fill="#4A90E2"/>
        <circle cx="50" cy="50" r="25" fill="white"/>
        <text x="50" y="55" font-family="Arial" font-size="14" fill="#4A90E2" text-anchor="middle">RAG</text>
    </svg>
    '''

    b64 = base64.b64encode(logo_svg.encode('utf-8')).decode()

    st.markdown(
        f"""
        <style>
            [data-testid="stHeader"] {{
                background-color: white;
            }}

            .logo-container {{
                display: flex;
                justify-content: center;
                margin-bottom: 2rem;
            }}

            .chat-container {{
                max-width: 800px;
                margin: 0 auto;
                padding: 2rem;
            }}

            .stChatMessage {{
                background-color: #ffffff;
                border-radius: 10px;
                padding: 1rem;
                margin: 0.5rem 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
        </style>
        <div class="logo-container">
            <img src="data:image/svg+xml;base64,{b64}" alt="Logo" width="100">
        </div>
        """,
        unsafe_allow_html=True
    )

def main():
    """Main Streamlit application"""
    st.set_page_config(page_title="Document Q&A System", layout="wide")
    add_logo()

    st.markdown('<h1 style="text-align: center;">Ask me anything about the fund</h1>', unsafe_allow_html=True)

    # Initialize the QA system if not already done
    if "qa_chain" not in st.session_state:
        with st.spinner("Initializing the system..."):
            st.session_state.qa_chain = initialize_qa_system()
            st.session_state.chat_history = []

    # Chat interface
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("What would you like to know?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response = st.session_state.qa_chain(
                {"question": prompt, "chat_history": st.session_state.chat_history}
            )
            st.markdown(response["answer"])
            st.session_state.messages.append({"role": "assistant", "content": response["answer"]})
            st.session_state.chat_history.append((prompt, response["answer"]))

    st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()
